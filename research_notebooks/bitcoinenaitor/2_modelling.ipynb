{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T17:31:49.704972Z",
     "start_time": "2025-02-14T17:31:49.701731Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This is necessary to recognize the modules\n",
    "import os\n",
    "import sys\n",
    "from decimal import Decimal\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['quote_asset_volume', 'n_trades', 'target', 'close_type', 'BBL_20_2.0',\n",
      "       'BBM_20_2.0', 'BBU_20_2.0', 'BBB_20_2.0', 'BBP_20_2.0', 'BBL_50_2.0',\n",
      "       'BBM_50_2.0', 'BBU_50_2.0', 'BBB_50_2.0', 'BBP_50_2.0', 'MACD_12_26_9',\n",
      "       'MACDh_12_26_9', 'MACDs_12_26_9', 'MACD_8_21_5', 'MACDh_8_21_5',\n",
      "       'MACDs_8_21_5', 'RSI_14', 'RSI_21', 'SMA_20', 'SMA_50', 'EMA_20',\n",
      "       'EMA_50', 'ATRr_14', 'STOCHk_14_3_3', 'STOCHd_14_3_3', 'ADX_14',\n",
      "       'DMP_14', 'DMN_14', 'open_ret', 'high_ret', 'low_ret', 'close_ret',\n",
      "       'buy_volume_ratio'],\n",
      "      dtype='object')\n",
      "37\n",
      "36\n",
      "Initial class distribution:\n",
      "close_type\n",
      "-1     66271\n",
      " 0    110564\n",
      " 1     61480\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Balanced class distribution:\n",
      "close_type\n",
      "-1    66271\n",
      " 0    63875\n",
      " 1    61480\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Training model...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.48      0.52      0.50     26337\n",
      "           0       0.48      0.80      0.61     25498\n",
      "           1       0.55      0.14      0.22     24816\n",
      "\n",
      "    accuracy                           0.49     76651\n",
      "   macro avg       0.51      0.49      0.44     76651\n",
      "weighted avg       0.50      0.49      0.44     76651\n",
      "\n",
      "\n",
      "Saving model and scaler...\n",
      "Model saved to: /home/dominhnhat/quants-lab/models/binance_BTC-USDT_1s_xgb_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "connector_name = \"binance\"\n",
    "trading_pair = \"BTC-USDT\"\n",
    "interval = \"1s\"\n",
    "\n",
    "df_with_features = pd.read_parquet('/home/dominhnhat/quants-lab/research_notebooks/bitcoinenaitor/data/features_df/binance|BTC-USDT|1s.parquet')\n",
    "print(df_with_features.columns)\n",
    "print(len(df_with_features.columns))\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = [col for col in df_with_features.columns if col not in ['timestamp', 'tl', 'stop_loss_time', \n",
    "                                                                       'take_profit_time', 'close_time', 'close_type',\n",
    "                                                                       'real_class', 'ret']]\n",
    "\n",
    "print(len(feature_columns))\n",
    "\n",
    "X = df_with_features[feature_columns]\n",
    "y = df_with_features['close_type']\n",
    "\n",
    "# Print initial class distribution\n",
    "print(\"Initial class distribution:\")\n",
    "print(y.value_counts().sort_index())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Get the size of the smaller classes\n",
    "target_size = df_with_features[df_with_features['close_type'] != 0].shape[0] // 2\n",
    "df_neg = df_with_features[df_with_features['close_type'] == -1]\n",
    "df_pos = df_with_features[df_with_features['close_type'] == 1]\n",
    "df_mid = df_with_features[df_with_features['close_type'] == 0].sample(n=target_size, random_state=42)\n",
    "\n",
    "# Combine the balanced dataset\n",
    "balanced_df = pd.concat([df_neg, df_mid, df_pos])\n",
    "\n",
    "X_balanced = balanced_df[feature_columns]\n",
    "y_balanced = balanced_df['close_type']\n",
    "\n",
    "# Print balanced distribution\n",
    "print(\"Balanced class distribution:\")\n",
    "print(y_balanced.value_counts().sort_index())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Split the data first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.4, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "# Initialize and train XGBoost\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced',\n",
    ")\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Print model performance\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and scaler\n",
    "print(\"\\nSaving model and scaler...\")\n",
    "model_path = os.path.join(root_path, \"models\", f\"{connector_name}_{trading_pair}_{interval}_xgb_model.joblib\")\n",
    "scaler_path = os.path.join(root_path, \"models\", f\"{connector_name}_{trading_pair}_{interval}_scaler.joblib\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "# Save both model and scaler\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['quote_asset_volume', 'n_trades', 'target', 'close_type', 'BBL_20_2.0',\n",
      "       'BBM_20_2.0', 'BBU_20_2.0', 'BBB_20_2.0', 'BBP_20_2.0', 'BBL_50_2.0',\n",
      "       'BBM_50_2.0', 'BBU_50_2.0', 'BBB_50_2.0', 'BBP_50_2.0', 'MACD_12_26_9',\n",
      "       'MACDh_12_26_9', 'MACDs_12_26_9', 'MACD_8_21_5', 'MACDh_8_21_5',\n",
      "       'MACDs_8_21_5', 'RSI_14', 'RSI_21', 'SMA_20', 'SMA_50', 'EMA_20',\n",
      "       'EMA_50', 'ATRr_14', 'STOCHk_14_3_3', 'STOCHd_14_3_3', 'ADX_14',\n",
      "       'DMP_14', 'DMN_14', 'open_ret', 'high_ret', 'low_ret', 'close_ret',\n",
      "       'buy_volume_ratio'],\n",
      "      dtype='object')\n",
      "Train size: 172463, Test size: 19163\n",
      "Train class counts: {-1: 59644, 0: 57487, 1: 55332}\n",
      "Test class counts: {-1: 6627, 0: 6388, 1: 6148}\n"
     ]
    }
   ],
   "source": [
    "df_balanced = balanced_df\n",
    "\n",
    "print(balanced_df.columns)\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df_balanced, test_size=0.1, random_state=42, stratify=df_balanced['close_type']\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
    "print(\"Train class counts:\", train_df['close_type'].value_counts().to_dict())\n",
    "print(\"Test class counts:\", test_df['close_type'].value_counts().to_dict())\n",
    "\n",
    "X_train = train_df[feature_columns]\n",
    "y_train = train_df['close_type'].copy()\n",
    "X_test  = test_df[feature_columns]\n",
    "y_test  = test_df['close_type'].copy()\n",
    "\n",
    "# Convert labels to 0,1,2 encoding for PyTorch\n",
    "label_mapping = {-1: 0, 0: 1, 1: 2}\n",
    "y_train_mapped = y_train.map(label_mapping).values\n",
    "y_test_mapped = y_test.map(label_mapping).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.drop4 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc5 = nn.Linear(64, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.drop4(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_train.shape[1]  # number of feature columns\n",
    "model = MLP(input_dim)\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=36, out_features=512, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (drop1): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (drop2): Dropout(p=0.2, inplace=False)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (drop3): Dropout(p=0.2, inplace=False)\n",
       "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu4): ReLU()\n",
       "  (drop4): Dropout(p=0.2, inplace=False)\n",
       "  (fc5): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19163, 36])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert training and testing data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_mapped, dtype=torch.long)\n",
    "X_test_tensor  = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor  = torch.tensor(y_test_mapped, dtype=torch.long)\n",
    "\n",
    "print(X_test_tensor.shape)\n",
    "\n",
    "# Create TensorDataset and DataLoader for training and testing\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "bsize = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=bsize, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=bsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Training Loss: 0.9372\n",
      "Epoch 2/150, Training Loss: 0.9313\n",
      "Epoch 3/150, Training Loss: 0.9275\n",
      "Epoch 4/150, Training Loss: 0.9228\n",
      "Epoch 5/150, Training Loss: 0.9212\n",
      "Epoch 6/150, Training Loss: 0.9171\n",
      "Epoch 7/150, Training Loss: 0.9142\n",
      "Epoch 8/150, Training Loss: 0.9086\n",
      "Epoch 9/150, Training Loss: 0.9068\n",
      "Epoch 10/150, Training Loss: 0.9019\n",
      "Epoch 11/150, Training Loss: 0.8993\n",
      "Epoch 12/150, Training Loss: 0.8964\n",
      "Epoch 13/150, Training Loss: 0.8933\n",
      "Epoch 14/150, Training Loss: 0.8902\n",
      "Epoch 15/150, Training Loss: 0.8877\n",
      "Epoch 16/150, Training Loss: 0.8820\n",
      "Epoch 17/150, Training Loss: 0.8823\n",
      "Epoch 18/150, Training Loss: 0.8781\n",
      "Epoch 19/150, Training Loss: 0.8743\n",
      "Epoch 20/150, Training Loss: 0.8727\n",
      "Epoch 21/150, Training Loss: 0.8691\n",
      "Epoch 22/150, Training Loss: 0.8667\n",
      "Epoch 23/150, Training Loss: 0.8661\n",
      "Epoch 24/150, Training Loss: 0.8645\n",
      "Epoch 25/150, Training Loss: 0.8588\n",
      "Epoch 26/150, Training Loss: 0.8578\n",
      "Epoch 27/150, Training Loss: 0.8551\n",
      "Epoch 28/150, Training Loss: 0.8544\n",
      "Epoch 29/150, Training Loss: 0.8532\n",
      "Epoch 30/150, Training Loss: 0.8511\n",
      "Epoch 31/150, Training Loss: 0.8494\n",
      "Epoch 32/150, Training Loss: 0.8456\n",
      "Epoch 33/150, Training Loss: 0.8447\n",
      "Epoch 34/150, Training Loss: 0.8424\n",
      "Epoch 35/150, Training Loss: 0.8391\n",
      "Epoch 36/150, Training Loss: 0.8388\n",
      "Epoch 37/150, Training Loss: 0.8367\n",
      "Epoch 38/150, Training Loss: 0.8357\n",
      "Epoch 39/150, Training Loss: 0.8318\n",
      "Epoch 40/150, Training Loss: 0.8331\n",
      "Epoch 41/150, Training Loss: 0.8305\n",
      "Epoch 42/150, Training Loss: 0.8289\n",
      "Epoch 43/150, Training Loss: 0.8280\n",
      "Epoch 44/150, Training Loss: 0.8269\n",
      "Epoch 45/150, Training Loss: 0.8242\n",
      "Epoch 46/150, Training Loss: 0.8239\n",
      "Epoch 47/150, Training Loss: 0.8238\n",
      "Epoch 48/150, Training Loss: 0.8199\n",
      "Epoch 49/150, Training Loss: 0.8181\n",
      "Epoch 50/150, Training Loss: 0.8198\n",
      "Epoch 51/150, Training Loss: 0.8178\n",
      "Epoch 52/150, Training Loss: 0.8155\n",
      "Epoch 53/150, Training Loss: 0.8130\n",
      "Epoch 54/150, Training Loss: 0.8117\n",
      "Epoch 55/150, Training Loss: 0.8122\n",
      "Epoch 56/150, Training Loss: 0.8121\n",
      "Epoch 57/150, Training Loss: 0.8086\n",
      "Epoch 58/150, Training Loss: 0.8066\n",
      "Epoch 59/150, Training Loss: 0.8093\n",
      "Epoch 60/150, Training Loss: 0.8060\n",
      "Epoch 61/150, Training Loss: 0.8064\n",
      "Epoch 62/150, Training Loss: 0.8043\n",
      "Epoch 63/150, Training Loss: 0.8032\n",
      "Epoch 64/150, Training Loss: 0.8030\n",
      "Epoch 65/150, Training Loss: 0.8011\n",
      "Epoch 66/150, Training Loss: 0.7996\n",
      "Epoch 67/150, Training Loss: 0.7979\n",
      "Epoch 68/150, Training Loss: 0.7951\n",
      "Epoch 69/150, Training Loss: 0.7957\n",
      "Epoch 70/150, Training Loss: 0.7936\n",
      "Epoch 71/150, Training Loss: 0.7928\n",
      "Epoch 72/150, Training Loss: 0.7921\n",
      "Epoch 73/150, Training Loss: 0.7936\n",
      "Epoch 74/150, Training Loss: 0.7919\n",
      "Epoch 75/150, Training Loss: 0.7907\n",
      "Epoch 76/150, Training Loss: 0.7883\n",
      "Epoch 77/150, Training Loss: 0.7903\n",
      "Epoch 78/150, Training Loss: 0.7876\n",
      "Epoch 79/150, Training Loss: 0.7868\n",
      "Epoch 80/150, Training Loss: 0.7866\n",
      "Epoch 81/150, Training Loss: 0.7831\n",
      "Epoch 82/150, Training Loss: 0.7840\n",
      "Epoch 83/150, Training Loss: 0.7841\n",
      "Epoch 84/150, Training Loss: 0.7833\n",
      "Epoch 85/150, Training Loss: 0.7809\n",
      "Epoch 86/150, Training Loss: 0.7801\n",
      "Epoch 87/150, Training Loss: 0.7775\n",
      "Epoch 88/150, Training Loss: 0.7788\n",
      "Epoch 89/150, Training Loss: 0.7780\n",
      "Epoch 90/150, Training Loss: 0.7770\n",
      "Epoch 91/150, Training Loss: 0.7781\n",
      "Epoch 92/150, Training Loss: 0.7749\n",
      "Epoch 93/150, Training Loss: 0.7774\n",
      "Epoch 94/150, Training Loss: 0.7774\n",
      "Epoch 95/150, Training Loss: 0.7733\n",
      "Epoch 96/150, Training Loss: 0.7736\n",
      "Epoch 97/150, Training Loss: 0.7713\n",
      "Epoch 98/150, Training Loss: 0.7722\n",
      "Epoch 99/150, Training Loss: 0.7702\n",
      "Epoch 100/150, Training Loss: 0.7704\n",
      "Epoch 101/150, Training Loss: 0.7694\n",
      "Epoch 102/150, Training Loss: 0.7667\n",
      "Epoch 103/150, Training Loss: 0.7673\n",
      "Epoch 104/150, Training Loss: 0.7673\n",
      "Epoch 105/150, Training Loss: 0.7661\n",
      "Epoch 106/150, Training Loss: 0.7647\n",
      "Epoch 107/150, Training Loss: 0.7667\n",
      "Epoch 108/150, Training Loss: 0.7637\n",
      "Epoch 109/150, Training Loss: 0.7628\n",
      "Epoch 110/150, Training Loss: 0.7631\n",
      "Epoch 111/150, Training Loss: 0.7654\n",
      "Epoch 112/150, Training Loss: 0.7611\n",
      "Epoch 113/150, Training Loss: 0.7601\n",
      "Epoch 114/150, Training Loss: 0.7603\n",
      "Epoch 115/150, Training Loss: 0.7591\n",
      "Epoch 116/150, Training Loss: 0.7593\n",
      "Epoch 117/150, Training Loss: 0.7562\n",
      "Epoch 118/150, Training Loss: 0.7564\n",
      "Epoch 119/150, Training Loss: 0.7581\n",
      "Epoch 120/150, Training Loss: 0.7580\n",
      "Epoch 121/150, Training Loss: 0.7566\n",
      "Epoch 122/150, Training Loss: 0.7579\n",
      "Epoch 123/150, Training Loss: 0.7564\n",
      "Epoch 124/150, Training Loss: 0.7565\n",
      "Epoch 125/150, Training Loss: 0.7548\n",
      "Epoch 126/150, Training Loss: 0.7530\n",
      "Epoch 127/150, Training Loss: 0.7526\n",
      "Epoch 128/150, Training Loss: 0.7549\n",
      "Epoch 129/150, Training Loss: 0.7503\n",
      "Epoch 130/150, Training Loss: 0.7490\n",
      "Epoch 131/150, Training Loss: 0.7537\n",
      "Epoch 132/150, Training Loss: 0.7499\n",
      "Epoch 133/150, Training Loss: 0.7513\n",
      "Epoch 134/150, Training Loss: 0.7490\n",
      "Epoch 135/150, Training Loss: 0.7476\n",
      "Epoch 136/150, Training Loss: 0.7480\n",
      "Epoch 137/150, Training Loss: 0.7468\n",
      "Epoch 138/150, Training Loss: 0.7456\n",
      "Epoch 139/150, Training Loss: 0.7466\n",
      "Epoch 140/150, Training Loss: 0.7471\n",
      "Epoch 141/150, Training Loss: 0.7429\n",
      "Epoch 142/150, Training Loss: 0.7443\n",
      "Epoch 143/150, Training Loss: 0.7450\n",
      "Epoch 144/150, Training Loss: 0.7437\n",
      "Epoch 145/150, Training Loss: 0.7454\n",
      "Epoch 146/150, Training Loss: 0.7423\n",
      "Epoch 147/150, Training Loss: 0.7426\n",
      "Epoch 148/150, Training Loss: 0.7407\n",
      "Epoch 149/150, Training Loss: 0.7418\n",
      "Epoch 150/150, Training Loss: 0.7416\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # cross-entropy for multi-class\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "# Training loop\n",
    "epochs = 150\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()  # set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)  # move data to device\n",
    "        \n",
    "        optimizer.zero_grad()             # reset gradients\n",
    "        outputs = model(batch_X)          # forward pass\n",
    "        loss = criterion(outputs, batch_y)  # compute loss\n",
    "        loss.backward()                   # backpropagation\n",
    "        optimizer.step()                  # update parameters\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}/{epochs}, Training Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.72      0.64      0.68      6627\n",
      "           0       0.71      0.76      0.73      6388\n",
      "           1       0.67      0.70      0.68      6148\n",
      "\n",
      "    accuracy                           0.70     19163\n",
      "   macro avg       0.70      0.70      0.70     19163\n",
      "weighted avg       0.70      0.70      0.70     19163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "model.eval()  # set model to evaluation mode\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        preds = outputs.argmax(dim=1)  # predicted class index for each sample\n",
    "        y_pred.append(preds.cpu().numpy())\n",
    "        y_true.append(batch_y.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "y_pred = np.concatenate(y_pred)\n",
    "y_true = np.concatenate(y_true)\n",
    "inv_mapping = {0: -1, 1: 0, 2: 1}\n",
    "y_pred_orig = [inv_mapping[i] for i in y_pred]\n",
    "y_true_orig = [inv_mapping[i] for i in y_true]\n",
    "\n",
    "# import random\n",
    "# y_pred_new = []\n",
    "# flip_prob = 0.35\n",
    "# for p in y_pred_orig:\n",
    "#     if random.random() < flip_prob:\n",
    "#         new_label = random.choice([c for c in [-1, 0, 1] if c != p])\n",
    "#         y_pred_new.append(new_label)\n",
    "#     else:\n",
    "#         y_pred_new.append(p)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true_orig, y_pred_orig, labels=[-1, 0, 1], target_names=[\"-1\", \"0\", \"1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /home/dominhnhat/quants-lab/research_notebooks/bitcoinenaitor/models/mlp.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = '/home/dominhnhat/quants-lab/research_notebooks/bitcoinenaitor/models/mlp.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172463, 36)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quants-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
